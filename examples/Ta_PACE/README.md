## FitSnap3 Tantalum example

This example will generate a potential for tantalum will be published soon.

#### Running this example:

To run this example, use the following command in this directory:

python3 -m fitsnap3 Ta.in

#### Files in this Directory

Ta.in 

Input file containing parameters to run FitSNAP and generate
the tantalum ACE potential using SVD

svd/

Reference folder for ACE potential training using SVD (153 descriptors)

lasso/

Reference folder for ace potential using LASSO regression (153 descriptors)

svd_bigbase/

Reference folder for ace potential using SVD with more descriptors than structures (734 descriptors).

ard_bigbase/

Reference folder for ace potential using ARD with more descriptors than structures (734 descriptors)
Training with these parameters produces a comparable potential to those in svd/ with 163 non-zero parameters

#### Files generated by example:

Ta_pot.acecoeff

ACE potential coefficient file that contains all the beta coefficients for 
this potential. With this file and the ccs.json file, a potential may be generated for LAMMPS

ccs.json 

File containing all of the generalized wigner-3j symbols needed to construct the ACE invariants. With this file and the .acecoeff file, a potential.ace may be generated with the write_pot.py script.

coupling_coefficients.ace

ML-PACE compatible file containing the generalized wigner-3j symbols as well as basis function parameters needed to define the ACE potential and run compute_pace in LAMMPS.

Ta_metrics.csv

Contains a variety of error metrics for all the training groups for this fit.


#### Important input parameters for this example
ACE descriptors up to arbitrary rank may be used. 
```
For rank 1 descriptors lmax has no effect (only radial dependence for 2-body descriptors)
For 2 <= rank <=3 it is recommended that lmax should not exceed 7.
For rank 4 descriptors, it is recommended that lmax should not exceed 3,
For 5 <=rank <= 7 it is recommended that lmax should not exceed 2. lmax of 2 for rank 7 descriptors will start to result in longer runtimes
For rank >= 8, it is recommended that lmax should not exceed 1.

Adding in more ranks, increasing lmax, and increasing nmax will increase the number of descriptors. This allows for more accurate models but with increased computational cost.

ranks = 1 2 3 4 5 6 7   : list of ACE descriptor ranks chosen for this potential (body order for the ace descriptors goes like [rank+1])
lmax =  1 5 4 2 1 1 1   : list of lmax quantum numbers per descriptor rank (lmax of rank 1 has no effect on rank 1 basis)
nmax = 22 5 4 2 1 1 1   : list of nmax quantum numbers per descriptor rank
nmaxbase = 22           : number of rank 1 functions (to be used in future updates to expand radial basis)
rcutfac = 4.604694451   : Radial cutoff (hyperparameters) chosen for this potential
lambda = 3.059235105    : lambda exponential decay factor in the Chebyshev exponential cosine basis from Drautz 2019 (controls how fast the scaled distance decays as r->rc)
type = Ta               : Chemical symbol for elements matching those in JSON file
wigner_flag = 1         : Flag to use wigner 3j coupling coefficients (generalized CG coefficients are not implemented in this version)
```

See docs/TEMPLATE.in for further information on input parameters

##### For LASSO regression training

Lasso regression applies an l1 norm penalty to the least squares cost function, and is useful for obtaining sparse solutions. 
See https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoRegression.html for more info.

```
solver = LASSO               : flag to tell FitSNAP to use lasso regression
alpha = 1.0E-4               : penalty for l1 norm term (defaults to 1e-8)
max_iter = 1000              : maximum number of iterations to minimize the LASSO cost function
```

Warning: increasing max_iter may significantly increase training time if the fit is unstable. (See ARD for a more stable sparse training method.)

##### For ARD (automatic relevence determination) regression training

ARDR is a Bayesian method that is good at obtaining sparse solutions to fitting problems. The prior distribution for each descriptor may be differrent, and the training is stabilized by shifting the priors towards 0. 
The ARDR cost function has l1 and l2 regularizing properties.
See https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ARDRegression.html for more info.

```
solver = ARD                 : flag to tell FitSNAP to use ARD regression
alphabig = 1e-12             : shape parameter for gamma distribution of priors depending on alpha
alphasmall = 1e-12           : rate parameter for gamma distribution of priors depending on alpha
lambdabig = 1e-6             : shape parameter for gamma distribution of priors depending on lambda (lambda the ARD hyperparameter not to be confused with the basis function lambda)
lambdasmall = 1e-6           : rate parameter for gamma distribution of priors depending on lambda 
threshold_lambda = 100000    : threshold for removing parameters (controls model sparsity)
max_iter = 1000              : maximum number of iterations to minimize ARD cost function (should not need to change)
```

#### Tantalum data from:

The JSON configurations used for this example are published in:

Thompson, A. P., Swiler, L. P., Trott, C. R., Foiles, S. M., & Tucker, G. J. (2015). 
Spectral neighbor analysis method for automated generation of quantum-accurate interatomic 
potentials. Journal of Computational Physics, 285, 316-330

The hyperparameters and ACE potential form will be published soon.

**Note to Developers: Make sure this example still reproduces the same results when modifying code**

